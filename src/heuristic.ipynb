{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to create RNN with input layer of dimension 301, and output layer of dimension 300.\n",
    "\n",
    "-Input layer: Concatenate word vector (300 elements) with the associated score (1 element)\n",
    "-Output layer: Guess the vector of the closest word (maybe find nearest neighbor, or approx. nearest neighbor?)\n",
    "\n",
    "Feed in last N guesses (or all guesses if n_guesses < N)\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Generate training, validation data:\n",
    "-choose word at random to be solution\n",
    "-choose 5 words at random and get similarity score with random word\n",
    "-feed in word + score into RNN one by one\n",
    "-output in JSON file\n",
    "2. Set up RNN with correct amount of dimensions\n",
    "3. Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in ./.venv/lib/python3.9/site-packages (4.3.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in ./.venv/lib/python3.9/site-packages (from gensim) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in ./.venv/lib/python3.9/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in ./.venv/lib/python3.9/site-packages (from gensim) (6.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: sklearn in ./.venv/lib/python3.9/site-packages (0.0.post4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.9/site-packages (8.0.6)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in ./.venv/lib/python3.9/site-packages (from ipywidgets) (4.0.7)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.9/site-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.9/site-packages (from ipywidgets) (8.11.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in ./.venv/lib/python3.9/site-packages (from ipywidgets) (3.0.7)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in ./.venv/lib/python3.9/site-packages (from ipywidgets) (6.22.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./.venv/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./.venv/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.4)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in ./.venv/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (8.1.0)\n",
      "Requirement already satisfied: appnope in ./.venv/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: tornado>=6.1 in ./.venv/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./.venv/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.6)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./.venv/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (23.0)\n",
      "Requirement already satisfied: pyzmq>=20 in ./.venv/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.2)\n",
      "Requirement already satisfied: nest-asyncio in ./.venv/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: backcall in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.14.0)\n",
      "Requirement already satisfied: pickleshare in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in ./.venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.38)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in ./.venv/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in ./.venv/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (6.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./.venv/lib/python3.9/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.9/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.9/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.venv/lib/python3.9/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (3.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U gensim\n",
    "!pip3 install -U sklearn\n",
    "!pip3 install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from answers import secretWords as answers\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from constants import PATH_TO_DATASET\n",
    "import torch.optim as optim\n",
    "import ipywidgets\n",
    "import heuristicrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATASET = \"~/Desktop/CS 4701/archive/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "gnews_model = gensim.models.KeyedVectors.load_word2vec_format(PATH_TO_DATASET, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = gnews_model[answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_entries, filename):\n",
    "  res = []\n",
    "  for i in range(num_entries):\n",
    "    # Select 7 random word vectors\n",
    "    random_values = random.sample(range(len(embeddings)), k=7)\n",
    "    vectors = embeddings[random_values]\n",
    "    guesses = vectors[0:5]\n",
    "    correct_guess = vectors[5]\n",
    "    next_guess = vectors[6]\n",
    "\n",
    "    # The corresponding strings\n",
    "    words = [answers[i] for i in random_values]\n",
    "    guessed_words = words[0:5]\n",
    "    correct_word = words[5]\n",
    "    next_word = words[6]\n",
    "\n",
    "    # Calculate cosine similarity between each guess and the correct guess\n",
    "    similarity_scores = [gnews_model.similarity(guessed_word, correct_word) for guessed_word in guessed_words]\n",
    "\n",
    "    # Format the past guesses as a list of pairs of word vectors and corresponding similarity scores\n",
    "    past_guesses = [{\"word\": word, \"similarity_score\": float(score * 100)} for word, score in zip(guessed_words, similarity_scores)]\n",
    "\n",
    "    # Create a JSON object with the past guesses and correct guess\n",
    "    entry = {\"past_guesses\": past_guesses, \"next_guess\": next_word, \"actual_score\": float(gnews_model.similarity(next_word, correct_word) * 100)}\n",
    "    res.append(entry)\n",
    "    \n",
    "  output = {\"entries\": res}\n",
    "  # Write output JSON to file\n",
    "  with open(f\"{filename}.json\", \"w\") as file:\n",
    "    json.dump(output, file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ENTRIES = 4000\n",
    "\n",
    "generate_data(NUM_ENTRIES, \"train\")\n",
    "generate_data(NUM_ENTRIES, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate similarity score to each word vector except the next guess (concatenate a 0)\n",
    "def get_data_from_json(res):\n",
    "    data = []\n",
    "    expected_scores = []\n",
    "    for entry in res:\n",
    "        new_entry = []\n",
    "        for guess in entry[\"past_guesses\"]:\n",
    "            guess_vector = gnews_model[guess[\"word\"]].tolist()\n",
    "            guess_cat = [guess_vector + [guess[\"similarity_score\"]]]\n",
    "            new_entry += guess_cat\n",
    "\n",
    "        next_guess_cat = [gnews_model[entry[\"next_guess\"]].tolist() + [0]]\n",
    "        new_entry += next_guess_cat\n",
    "        data += [new_entry]\n",
    "        expected_scores += [entry[\"actual_score\"]]\n",
    "\n",
    "    return data, expected_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.json\", \"r\") as file:\n",
    "  f = json.load(file)\n",
    "  train_data, train_expected_scores = get_data_from_json(f[\"entries\"])\n",
    "\n",
    "with open(\"val.json\", \"r\") as file:\n",
    "  f = json.load(file)\n",
    "  val_data, val_expected_scores = get_data_from_json(f[\"entries\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "11.763811111450195\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(train_expected_scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataset(Dataset):\n",
    "    def __init__(self, guesses, scores):\n",
    "        \"\"\"\n",
    "        Loads in the word dataset as tensors.\n",
    "\n",
    "        Args:\n",
    "          guesses: groups of 5 guesses concatenated with their scores, and 1 guess concatenated with 0\n",
    "        \"\"\"\n",
    "        self.guesses = torch.tensor(guesses)\n",
    "        self.scores = torch.tensor(scores)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.guesses)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get the model input for the given index.\n",
    "        \"\"\"\n",
    "        return self.guesses[index], self.scores[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(data, scores, batch_size=1, shuffle=False):\n",
    "    dataset = WordDataset(data, scores)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_data_loaders(train_data, train_expected_scores, shuffle=True, batch_size=3)\n",
    "val_loader = get_data_loaders(val_data, val_expected_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    batch = 0\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for (input_batch, expected_out) in tqdm(train_loader, leave=False, desc=\"Training Batches\"):\n",
    "        optimizer.zero_grad()\n",
    "        batch += 1\n",
    "        output = model(input_batch)\n",
    "\n",
    "        loss = model.compute_loss(output, expected_out)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Average loss: {total_loss/batch}\")\n",
    "    return total_loss/batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(model, val_loader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    total_loss = 0\n",
    "    for(input_batch, expected_out) in tqdm(val_loader, leave=False, desc=\"Validation\"):\n",
    "        output = model(input_batch)\n",
    "        loss = model.compute_loss(output, expected_out)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Average loss: {total_loss/len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, train_loader, learning_rate=0.001):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=.01)\n",
    "    for epoch in trange(num_epochs, desc=\"Epochs\"):\n",
    "        cur_loss = train_epoch(model, train_loader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(num_epochs, model, val_loader):\n",
    "    for epoch in trange(num_epochs, desc=\"Epochs\"):\n",
    "        evaluate_epoch(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "\tif isinstance(m, nn.Linear):\n",
    "\t\tnn.init.xavier_uniform_(m.weight)\n",
    "\t\tnn.init.constant_(m.bias, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9615f20ec3d34b33bc0976b208ed5ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3d5389107d415686cc1bed2dc74773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 5.862300193023825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0dab3218cb4cd8a80730461dcdbf8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 5.569367382815038\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a28e51be261427a9ce44f5823b80567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 5.317144947799518\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b3753e960f4d409cee78a17586df7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 4.884453966364689\n"
     ]
    }
   ],
   "source": [
    "# torch.Size([2000, 6, 301])\n",
    "\n",
    "semantle_rnn = heuristicrnn.RNN(301, 200, 1, 2)\n",
    "semantle_rnn.apply(weight_init)\n",
    "train(4, semantle_rnn, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantle_rnn.save_model(\"heuristicrnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantle_rnn = heuristicrnn.RNN(301, 200, 1, 2)\n",
    "semantle_rnn.load_model(\"heuristic-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8bc19a3e8f4789b415b00b2303bf78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7123ee6acafa4ba8bfa2267348b52206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 6.032291997164488\n"
     ]
    }
   ],
   "source": [
    "evaluate(1, semantle_rnn, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantle_rnn.save_model(\"heuristic-model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
